# -*- coding: utf-8 -*-
"""Youssef FAOUZI - House Prices

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_65P08nYI1r1WJN8vgTMW40eTg0B7MsX

# **Sommaire**

* Loading the Data                                                                            
* Data visualization and
  cleaning
* Les valeurs manquantes 

* Feature Engineering

* Models

 * Définir Train/Test

 * Linear regression
 
 * Decision Tree Regression

 * Random Forest Regression

 * xgboost

 * Gradient Boosting Regression / Parameter tuning

* Sumbission 

============================================       Fait par : Youssef FAOUZI    
============================================    Encadré par : M. ALEXIS PERRIER

##**Loading the Data**
"""

import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

train=pd.read_csv("https://drive.google.com/uc?export=download&id=1XJ7_LNUEXz8rRyIsxSghQi5RI0bef9Zr")
test=pd.read_csv("https://drive.google.com/uc?export=download&id=1jA0gcJcln26zg0QXivz26_F_8CNSBni0")
submission=pd.read_csv('https://drive.google.com/uc?export=download&id=1ZLQLEIw8XfG_-xvHFvMnoli4ly8Cevud')

"""##**Data visualization and cleaning**"""

train.head()

train.describe()

train.shape, test.shape

#Different types of the features
train.dtypes

train.shape

# Descrption de la variable cible 
train.SalePrice.describe()

#Histogramme de la vraible cible 

plt.figure(figsize=(12, 9)) 
plt.xlabel("Sale price", fontsize=16)  
plt.ylabel("Count", fontsize=16)
p=plt.hist(train.SalePrice.values ,  color="#3F5D7D", bins=100, )
p.index

train.corr()['SalePrice']

#la carte des correlations 

corrmat = train.corr()
f, ax = plt.subplots(figsize=(20, 15))
sns.set(font_scale=1.45)
sns.heatmap(corrmat, square=True,cmap='coolwarm');

correlations = corrmat["SalePrice"].sort_values(ascending=False)
mostCorrelated = correlations.index[0:10]
mostCorrelated

sns.pairplot(train[mostCorrelated], size = 2.5)
plt.show();

# salePrice en fct des anneés de vente
sns.relplot(x="YearBuilt", y="SalePrice", data = train)

"""##**Les valeurs manquantes**"""

#nombre des valeurs manquantes dans chaque colonne
train_null =train.isnull().sum(); test_null=test.isnull().sum() 
Nan= pd.concat([train_null, test_null], axis=1, keys=["Train", "Test"]) ;Nan

#repérer les colonnes avec des valeurs manquantes

many_nan = Nan[Nan.sum(axis=1) > 200] 
few_nan = Nan[Nan.sum(axis=1)>0 ][ Nan.sum(axis=1)<= 200] 
many_nan

# supprimer les colonnes vides 

train.drop(["LotFrontage","MiscFeature","Alley","FireplaceQu","PoolQC","Fence"], axis=1, inplace=True)
test.drop(["LotFrontage","MiscFeature","Alley","FireplaceQu","PoolQC","Fence"], axis=1, inplace=True)

few_nan

"""variable catégorielle `"MasVnrType"`"""

# MasVnrType est une variable catégorielle

train.MasVnrType.value_counts()

# remplacer les valeurs manquantes par None 
train["MasVnrType"].fillna("None", inplace=True)
test["MasVnrType"].fillna("None", inplace=True)



"""passons donc aux autres variables"""

types_train = train.dtypes 
num_train = types_train[(types_train == int) | (types_train == float)] 
cat_train = types_train[types_train == object]


#meme chose pour le test
types_test=test.dtypes
num_test = types_test[(types_test == int) | (types_test == float)] 
cat_test = types_test[types_test == object]

# les colonnes numériques  

numerical_cols_train = list(num_train.index)
numerical_cols_test = list(num_test.index) 



# remplir les valeurs manquantes par la moyenne : 

for i in numerical_cols_test: 
  train[i].fillna(train[i].mean(), inplace=True)
  test[i].fillna(test[i].mean(), inplace=True)
train[numerical_cols_train].isnull().sum()

# les colonnes catégorielles  

categorical_cols_train = list(cat_train.index)
categorical_cols_test = list(cat_test.index) 

#liste des colonnes catégorielles avec des valeurs manquantes
cat_nan_train=[]
for i in categorical_cols_train :
 if i in  few_nan.index : 
    cat_nan_train.append(i)  

cat_nan_test=[]
for i in categorical_cols_test :
 if i in  few_nan.index : 
    cat_nan_test.append(i)

#remplacer les valeurs manquantes par le terme le plus commun

for i in cat_nan_train: 
    k=cat_nan_train.index(i)
    train[i].fillna(list(train[cat_nan_test[k]].value_counts().index)[0], inplace=True) 
    test[i].fillna(list(test[cat_nan_test[k]].value_counts().index)[0], inplace=True)

train[cat_nan_train].isnull().sum()

"""vérifions c'est il y'a encore de valeurs manquantes"""

train_null =train.isnull().sum(); test_null=test.isnull().sum() 
Nan= pd.concat([train_null, test_null], axis=1, keys=["Train", "Test"]) ;Nan
few_nan = Nan[Nan.sum(axis=1)>0 ][ Nan.sum(axis=1)<= 200] 

few_nan

"""Ainsi nous avons reglé le problème des valeurs manquantes

##**Feature Engineering**

Pour rendre la variable cible gaussienne nous avons ajouté log(SalePrice) pour la base données
"""

train["LogPrice"] = np.log(train["SalePrice"]) 

plt.figure(figsize=(6,4)) 
plt.xlabel("Sale price", fontsize=10)  
plt.ylabel("Count", fontsize=11)
pp=plt.hist(train.LogPrice.values ,  color="#3F5D7D", bins=40 )
pp.index

"""Encoding des variables catégorielles"""

# label encoding pour les variables catégorielles : 
from sklearn import preprocessing
categorical_cols_train 
for i in categorical_cols_train :
 le = preprocessing.LabelEncoder() 
 le.fit(train[i])
 train[i] = le.fit_transform(train[i]) 
 test[i] = le.fit_transform(test[i])

train[categorical_cols_train[0]].value_counts()

"""**Nouvelles colonnes**"""

# let's try training the models after adding new features to the data 

new_train = train.copy()

new_train['TotalSF']=new_train['TotalBsmtSF'] + new_train['1stFlrSF'] + new_train['2ndFlrSF']

new_train['Total_Bathrooms'] = (new_train['FullBath'] + (0.5 * new_train['HalfBath']) +
                               new_train['BsmtFullBath'] + (0.5 * new_train['BsmtHalfBath']))

new_train['Total_porch_sf'] = (new_train['OpenPorchSF'] + new_train['3SsnPorch'] +
                              new_train['EnclosedPorch'] + new_train['ScreenPorch'] +
                              new_train['WoodDeckSF'])

new_test = test.copy()

new_test['TotalSF']=new_test['TotalBsmtSF'] + new_test['1stFlrSF'] + new_test['2ndFlrSF']

new_test['Total_Bathrooms'] = (new_test['FullBath'] + (0.5 * new_test['HalfBath']) +
                               new_test['BsmtFullBath'] + (0.5 * new_test['BsmtHalfBath']))

new_test['Total_porch_sf'] = (new_test['OpenPorchSF'] + new_test['3SsnPorch'] +
                              new_test['EnclosedPorch'] + new_test['ScreenPorch'] +
                              new_test['WoodDeckSF'])  



new_train['haspool'] = new_train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
new_train['has2ndfloor'] = new_train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
new_train['hasgarage'] = new_train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
new_train['hasbsmt'] = new_train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
new_train['hasfireplace'] = new_train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)

new_test['haspool'] = new_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
new_test['has2ndfloor'] = new_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
new_test['hasgarage'] = new_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
new_test['hasbsmt'] = new_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
new_test['hasfireplace'] = new_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)

"""##**Models**"""

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import datasets, linear_model, metrics
from sklearn.metrics import mean_squared_log_error
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV

"""##**Définir Train/Test**"""

from sklearn.model_selection import train_test_split #to create validation data set

X_train= new_train.drop(["SalePrice","LogPrice"], axis=1)
y_train=new_train.LogPrice 
 
X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets 



#fct pour calculer RMSLE et R2
def printScore(predictions,y):
  RMSLE=np.sqrt(mean_squared_log_error( y, predictions ))
  R2=r2_score(y,predictions)
  print(f"le RMSLE est {RMSLE}")
  print(f"le R2 est {R2}")

"""##**Linear regression**"""

mdl = LinearRegression()
mdl.fit(X_training,y_training)
ypred =  mdl.fit(X_training,y_training).predict(X_valid)
submissionPr_lr =  mdl.fit(X_training,y_training).predict(test)

print(f" R^2 Train {mdl.score(X_train, y_train)} " ) 
printScore(ypred , y_valid) 

#score kaggle 0,144

"""##**Decision Tree Regression**"""

dtreg = DecisionTreeRegressor(random_state = 100)
parameters_dtr = {"criterion" : ["mse", "friedman_mse", "mae"], "splitter" : ["best", "random"], "min_samples_split" : [2, 3, 5, 10], 
                  "max_features" : ["auto", "log2"]}
grid_dtr = GridSearchCV(dtreg, parameters_dtr, verbose=1, scoring="r2")
grid_dtr.fit(X_training, y_training) 

dtr = grid_dtr.best_estimator_
dtreg.fit(X_training, y_training)
dtr_pred = dtreg.predict(X_valid) 
submissionPr_dtr=dtreg.predict(test) 

print(f" R^2 Train {dtr.score(X_train, y_train)} " ) 
printScore(dtr_pred , y_valid) 

#score kaggle 0,23

"""## **Random Forest Regression**"""

rfr = RandomForestRegressor()
paremeters_rf = {"n_estimators" : [5, 10, 15, 20], "criterion" : ["mse" , "mae"], "min_samples_split" : [2, 3, 5, 10], 
                 "max_features" : ["auto", "log2"]}
grid_rf = GridSearchCV(rfr, paremeters_rf, verbose=1, scoring="r2")
grid_rf.fit(X_training, y_training) 
rf = grid_rf.best_estimator_
rf.fit(X_training, y_training)
rf_pred = rf.predict(X_valid) 
submissionPr_rf=rf.predict(test)

print(f" R^2 Train {rf.score(X_train, y_train)} " ) 
printScore(rf_pred , y_valid) 

#score kaggle 0,15

"""##**xgboost**"""

xgboost = XGBRegressor(learning_rate=0.01,n_estimators=20000,
                                     max_depth=3, min_child_weight=0,
                                     gamma=0, subsample=0.7,
                                     colsample_bytree=0.7,
                                     objective='reg:linear', nthread=-1,
                                     scale_pos_weight=1, seed=27,
                                     reg_alpha=0.006)
xgb = xgboost.fit(X_training, y_training) 
xgb_pred = xgb.predict(X_valid)  
submissionPr_xgb=xgb.predict(new_test)

print(f" R^2 Train {xgb.score(X_train, y_train)} " ) 
printScore(xgb_pred , y_valid)  

#score kaggle 0,138 avant l'ajout des nouvelles colonnes 
#score kaggle 0,131 aprés l'ajout des nouvelles colonnes

"""## **Gradient Boosting Regression / Parameter tuning**"""

from sklearn import ensemble

params = {'n_estimators': 1000,
          'max_depth': 5,
          'min_samples_split': 5,
          'learning_rate': 0.01,
          'loss': 'ls','max_features' : 20}
reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_training, y_training) 
reg_pred=reg.predict(X_valid)
submissionPr_reg=reg.predict(new_test)

print(f" R^2 Train {reg.score(X_train, y_train)} " ) 
printScore(reg_pred , y_valid)  

#score kaggle 0,13

"""**learning_rate**"""

from sklearn.metrics import mean_squared_error

learning_rates = [0.75 ,0.5, 0.25, 0.1, 0.05, 0.01]
r2_results = []
rmse_results = []
for eta in learning_rates:
    model = ensemble.GradientBoostingRegressor(learning_rate=eta)
    model.fit(X_training, y_training)
    y_pred = model.predict(X_valid)
    r2_clf = r2_score(y_valid, y_pred)
    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))
    r2_results.append(r2_clf)
    rmse_results.append(rmse_clf)
    
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(learning_rates, r2_results, 'b', label='R^2')
line2, = plt.plot(learning_rates, rmse_results, 'r', label='RMSE')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('learning_rates')
plt.show()

"""learning_rate optimal est: 0,05

**N_estimators**
"""

n_estimators = [1, 2, 16, 32, 64, 100, 200]
r2_results = []
rmse_results = []

for estimator in n_estimators:
    model = ensemble.GradientBoostingRegressor(n_estimators=estimator)
    model.fit(X_training, y_training)
    y_pred = model.predict(X_valid)
    r2_clf = r2_score(y_valid, y_pred)
    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))
    r2_results.append(r2_clf)
    rmse_results.append(rmse_clf)
    
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(n_estimators, r2_results, 'b', label='R^2')
line2, = plt.plot(n_estimators, rmse_results, 'r', label='RMSE')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('n_estimators')
plt.show()

"""100 est le nombre optimal des arbres pour notre modèle

**Max_depth**
"""

max_depths = np.linspace(1, 10, 10, endpoint=True)
r2_results = []
rmse_results = []

for max_depth in max_depths:
    model = ensemble.GradientBoostingRegressor(max_depth=max_depth)
    model.fit(X_training, y_training)
    y_pred = model.predict(X_valid)
    r2_clf = r2_score(y_valid, y_pred)
    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))
    r2_results.append(r2_clf)
    rmse_results.append(rmse_clf)
    
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(max_depths, r2_results, 'b', label='R^2')
line2, = plt.plot(max_depths, rmse_results, 'r', label='RMSE')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('max_depths')
plt.show()

"""nous allons choisir 5 comme max_dephts

**Min_samples_split**
"""

min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)
r2_results = []
rmse_results = []

for min_samples_split in min_samples_splits:
    model = ensemble.GradientBoostingRegressor(min_samples_split=min_samples_split)
    model.fit(X_training, y_training)
    y_pred = model.predict(X_valid)
    r2_clf = r2_score(y_valid, y_pred)
    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))
    r2_results.append(r2_clf)
    rmse_results.append(rmse_clf)
    
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(min_samples_splits, r2_results, 'b', label='R^2')
line2, = plt.plot(min_samples_splits, rmse_results, 'r', label='RMSE')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('min_samples_splits')
plt.show()

"""0,25 est optimal

**Min_samples_leaf**
"""

min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)
r2_results = []
rmse_results = []

for min_samples_leaf in min_samples_leafs:
    model = ensemble.GradientBoostingRegressor(min_samples_leaf=min_samples_leaf)
    model.fit(X_training, y_training)
    y_pred = model.predict(X_valid)
    r2_clf = r2_score(y_valid, y_pred)
    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))
    r2_results.append(r2_clf)
    rmse_results.append(rmse_clf)
    
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(min_samples_leafs, r2_results, 'b', label='R^2')
line2, = plt.plot(min_samples_leafs, rmse_results, 'r', label='RMSE')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('min_samples_leafs')
plt.show()

"""**max_features**"""

max_features = list(range(1,30))
r2_results = []
rmse_results = []

for max_feature in max_features:
    model = ensemble.GradientBoostingRegressor(max_features=max_feature)
    model.fit(X_training, y_training)
    y_pred = model.predict(X_valid)
    r2_clf = r2_score(y_valid, y_pred)
    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))
    r2_results.append(r2_clf)
    rmse_results.append(rmse_clf)
    
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(max_features, r2_results, 'b', label='R^2')
line2, = plt.plot(max_features, rmse_results, 'r', label='RMSE')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('max_features')
plt.show()

"""## **Sumbission**"""

from google.colab import files
submission=pd.read_csv(io.BytesIO(data_to_load['sample_submission.csv']))
submission['SalePrice']=np.exp(submissionPr_xgb)

submission.to_csv('sub.csv', sep = ',', index = False) 
files.download('sub.csv')